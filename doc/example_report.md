# Example Report: Production Server Performance Analysis

> **melisai** | Profile: `deep` (60s) | Tier 1 + BCC Tools

## Summary

| Metric | Value | Status |
|--------|-------|--------|
| **Health Score** | **32 / 100** | ðŸ”´ Critical |
| Uptime | 16 days | â€” |
| Kernel | 6.8.0-94-generic (Ubuntu 24.04) | âœ… |
| CPU | 8 cores, 55% util, **14.5% iowait** | âš ï¸ |
| RAM | 20 GB / 31 GB (62%) | âœ… |
| Disk | **97.9% utilization**, **113.9 ms latency** | ðŸ”´ |
| Load Average | 6.25 / 5.77 / 5.79 | âš ï¸ |

### Anomalies

| Severity | Message |
|----------|---------|
| ðŸ”´ CRITICAL | Disk utilization: **97.9%** |
| ðŸ”´ CRITICAL | Disk avg I/O latency: **113.9 ms** |
| âš ï¸ WARNING | CPU iowait: **14.5%** |
| âš ï¸ WARNING | CPU PSI pressure: **13.0%** |
| âš ï¸ WARNING | I/O PSI pressure: **17.5%** |

### USE Metrics

| Resource | Utilization | Saturation | Errors |
|----------|-------------|------------|--------|
| CPU | 55.3% | 0% | 0 |
| Disk | **97.9%** | **15%** | 0 |
| Memory | 61.6% | 0% | 0 |
| Network | 0% | **246%** | 0 |

---

## BCC Tool Data

### biolatency â€” Block I/O Latency Distribution (per-disk)

```
disk = sda (HDD, rotational):
     64 -  127 Âµs : â–ˆâ–ˆâ–ˆâ–ˆ  131
    128 -  255 Âµs : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  2565  â† bulk I/O
    256 -  511 Âµs : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  583
    512 - 1023 Âµs : â–ˆâ–ˆâ–ˆâ–ˆ  280
   1024 - 2047 Âµs : â–ˆ  99
   8192 -16383 Âµs : â–ˆ  81       â† HDD head seeking
  16384 -32767 Âµs :    20
  32768 -65535 Âµs :    12       â† heavy delays
  65536 -131ms    :    13       â† message broker fsync
 131072 -262ms    :     2       â† extreme tail latency
```

**P99 â‰ˆ 65 ms, Max = 262 ms**. The long tail is caused by message broker fsync operations competing with container log I/O on the same HDD RAID.

### ext4slower â€” Slow Filesystem Operations (>1ms)

| Time | Process | PID | Type | Latency (ms) | File | Analysis |
|------|---------|-----|------|-------------|------|----------|
| 08:36:14 | dockerd | 1095 | W | 25.33 | overlay2-log | Docker writing container JSON logs |
| 08:36:14 | flb-pipeline | 3132368 | R | 7.50 | overlay2-log | Log shipper reading same file |
| 08:36:15 | kafka-raft-io | 5929 | S (fsync) | **136.49** | kafka-commit.log | **Broker fsync â€” blocks entire I/O queue** |
| 08:36:15 | dockerd | 1095 | W | **114.25** | overlay2-log | Docker log write stalled behind fsync |
| 08:36:15 | flb-pipeline | 3132368 | R | **115.79** | overlay2-log | Log reader stalled behind fsync |
| 08:36:15 | kafka-raft-io | 5929 | S (fsync) | **261.27** | kafka-commit.log | **Worst-case fsync: 261 ms** |

**Clear pattern**: message broker fsync â†’ blocks HDD â†’ cascading stalls for dockerd and log shippers.

### runqlat â€” CPU Run Queue Latency Distribution

```
      0 -    1 Âµs : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  363557  â† most tasks scheduled fast
     64 -  127 Âµs : â–ˆ  10770
    512 - 1023 Âµs :     3164
   1024 - 2047 Âµs :     1703    â† 1-2ms CPU wait
   2048 - 4095 Âµs :     1045
  32768 -65535 Âµs :      154    â† 32-65ms scheduling delay
```

**154 tasks** waited 32-65 ms for CPU in a 10-second window. Caused by 8 cores being saturated at 55% average (with bursts from containers using 265% CPU total).

### cachestat â€” Page Cache effectiveness

```
    HITS   MISSES  DIRTIES HITRATIO
       0        0     1815    0.00%     â† zero cache hits
       0        0     1769    0.00%
       0        0     1908    0.00%
```

**0% cache hit ratio** with ~1800 dirty pages/sec. All I/O is write-dominant â€” page cache provides no read caching benefit.

---

## Root Cause Chain

```
  Message Broker fsync (136-261 ms per commit)
          â”‚
          â–¼
  HDD I/O queue saturated (97.9% utilization)
          â”‚
          â”œâ”€â”€ dockerd: writing container logs (25-114 ms wait)
          â”‚          â”‚
          â”‚          â””â”€â”€ Container stdout blocks â†’ app stalls
          â”‚
          â”œâ”€â”€ Log shipper: reading container logs (5-115 ms wait)
          â”‚
          â””â”€â”€ CPU iowait rises to 14.5%
                     â”‚
                     â–¼
              Tasks wait in run queue (up to 65 ms)
                     â”‚
                     â–¼
              TCP responses delayed â†’ retransmissions (246% saturation)
                     â”‚
                     â–¼
              ðŸ”´ Application latency increases
```

---

## Recommendations

### Immediate (reduce latency 3-5x)

| # | Action | Command | Effect |
|---|--------|---------|--------|
| 1 | Limit Docker log size | `{"log-opts": {"max-size": "10m", "max-file": "3"}}` in daemon.json | Reduce dockerd + log shipper I/O |
| 2 | Switch I/O scheduler to BFQ | `echo bfq > /sys/block/sda/queue/scheduler` | Prioritize short I/O over broker fsync |
| 3 | Reduce broker fsync frequency | `log.flush.interval.ms=5000` | Fewer fsync = less I/O blocking |

### Strategic

| # | Action | Rationale |
|---|--------|-----------|
| 4 | Move broker data to SSD | fsync on SSD: <1ms vs 96-261ms on HDD |
| 5 | Tune `vm.dirty_ratio=5` | Faster writeback, smaller I/O queue |
| 6 | Consider additional CPU cores | 8 cores at 55% avg with 265% container burst |

---

*This report was generated by `melisai collect --profile deep --ai-prompt`. The `ai_context.prompt` field in the JSON report contains a ready-to-use prompt for AI-driven analysis.*
